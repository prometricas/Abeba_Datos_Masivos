{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlC7/AcxuWiTnmFsWc4oAt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prometricas/Abeba_Datos_Masivos/blob/main/puntosEquipo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Instalación de Java y Hadoop**"
      ],
      "metadata": {
        "id": "yJHa4RzjhMa_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Instalar JDK de Java**"
      ],
      "metadata": {
        "id": "GQg7tQjVuobh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y openjdk-11-jdk-headless -qq > /dev/null"
      ],
      "metadata": {
        "id": "82PB4Zodu93s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Instalar Hadoop**"
      ],
      "metadata": {
        "id": "84JwNdLxwgQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargar Hadoop\n",
        "!wget https://downloads.apache.org/hadoop/common/hadoop-3.4.2/hadoop-3.4.2.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoFWj1DUwuPz",
        "outputId": "bb946cda-4752-4958-c6f7-0ce4126d5a2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-01-11 14:43:55--  https://downloads.apache.org/hadoop/common/hadoop-3.4.2/hadoop-3.4.2.tar.gz\n",
            "Resolving downloads.apache.org (downloads.apache.org)... 135.181.214.104, 88.99.208.237, 2a01:4f9:3a:2c57::2, ...\n",
            "Connecting to downloads.apache.org (downloads.apache.org)|135.181.214.104|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1065831750 (1016M) [application/x-gzip]\n",
            "Saving to: ‘hadoop-3.4.2.tar.gz’\n",
            "\n",
            "hadoop-3.4.2.tar.gz 100%[===================>]   1016M  25.3MB/s    in 41s     \n",
            "\n",
            "2026-01-11 14:44:37 (24.8 MB/s) - ‘hadoop-3.4.2.tar.gz’ saved [1065831750/1065831750]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Descomprimir y mover a carpeta de usuario\n",
        "!tar -xzf hadoop-3.4.2.tar.gz\n",
        "!mv hadoop-3.4.2/ /usr/local/\n",
        "\n",
        "# Mostrar los archivos de carpeta local:\n",
        "!ls /usr/local"
      ],
      "metadata": {
        "id": "QxAAWIYeI6KP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfcccf7b-6e12-40d3-a186-b0c1550e1ce1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bin    dist_metrics.pxd  games\t       include\tlibexec     man  sbin\tsrc\n",
            "colab  etc\t\t hadoop-3.4.2  lib\tLICENSE.md  opt  share\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Definir variables de entorno**\n"
      ],
      "metadata": {
        "id": "8tm2Ds-HJ6bF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
        "os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop-3.4.2\"\n",
        "os.environ[\"PATH\"] += os.pathsep + \"/usr/local/hadoop-3.4.2/bin\"\n",
        "!hadoop version"
      ],
      "metadata": {
        "id": "bl45zkpNKHI1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70d8476c-1234-453b-8a0a-022de357a9ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hadoop 3.4.2\n",
            "Source code repository https://github.com/apache/hadoop.git -r 84e8b89ee2ebe6923691205b9e171badde7a495c\n",
            "Compiled by ahmarsu on 2025-08-20T10:30Z\n",
            "Compiled on platform linux-x86_64\n",
            "Compiled with protoc 3.23.4\n",
            "From source with checksum fa94c67d4b4be021b9e9515c9b0f7b6\n",
            "This command was run using /usr/local/hadoop-3.4.2/share/hadoop/common/hadoop-common-3.4.2.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. `Importar base de datos`**"
      ],
      "metadata": {
        "id": "15V6Nt-khqRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar archivo\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "# Verificar\n",
        "!ls -lah\n",
        "!head -n 5 resultados_futbol.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "collapsed": true,
        "id": "uSQQLZH4h7mM",
        "outputId": "abc14be0-9d8d-4d42-9455-461bc617bb89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d2bdc014-7b9a-42a1-aaba-d27bd1934648\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d2bdc014-7b9a-42a1-aaba-d27bd1934648\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving resultados_futbol.csv to resultados_futbol.csv\n",
            "total 1018M\n",
            "drwxr-xr-x 1 root root  4.0K Jan 11 14:58 .\n",
            "drwxr-xr-x 1 root root  4.0K Jan 11 14:33 ..\n",
            "drwxr-xr-x 4 root root  4.0K Dec 11 14:34 .config\n",
            "-rw-r--r-- 1 root root 1017M Oct  4 09:30 hadoop-3.4.2.tar.gz\n",
            "drwxr-xr-x 2 root root  4.0K Jan 11 14:57 .ipynb_checkpoints\n",
            "-rw-r--r-- 1 root root  557K Jan 11 14:58 resultados_futbol.csv\n",
            "drwxr-xr-x 1 root root  4.0K Dec 11 14:34 sample_data\n",
            ",Season,Game,Score,Teams,Home Team,Away Team\n",
            "0,2000-2001,1,1-2,Mallorca - Real Madrid,Mallorca,Real Madrid\n",
            "1,2000-2001,1,1-2,Valencia - Racing,Valencia,Racing\n",
            "2,2000-2001,1,1-0,Athletic - Real Betis,Athletic,Real Betis\n",
            "3,2000-2001,1,0-2,Atlético - Rayo Vallecano,Atlético,Rayo Vallecano\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Crear scripts: *Mapper, Combiner, Reducer***\n"
      ],
      "metadata": {
        "id": "W0ch-yv0j4ia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Mapper**\n",
        "* Lee cada partido del CSV, calcula los puntos de local y visitante, y emite pares <equipo, puntos>.\n",
        "* Omite la cabecera y cualquier línea incompleta para evitar ruido en el cálculo.\n"
      ],
      "metadata": {
        "id": "riFd5TgFlIoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mapperPuntosEquipo.py\n",
        "#!/usr/bin/python3\n",
        "import sys\n",
        "import csv\n",
        "import re\n",
        "\n",
        "pat_equipo = re.compile(r\"\\s*[-–]\\s*\")  # Soportar '-' y '–' cuando toca separar equipos.\n",
        "\n",
        "for linea in sys.stdin:\n",
        "    linea = linea.strip()\n",
        "    if not linea:\n",
        "        continue\n",
        "\n",
        "    fila = next(csv.reader([linea]))\n",
        "\n",
        "    # Saltar la cabecera típica del CSV\n",
        "    if len(fila) >= 2 and fila[1].strip().lower() == \"season\":\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        # Priorizo Home Team / Away Team si vienen en el CSV (en este dataset vienen).\n",
        "        if len(fila) >= 7:\n",
        "            score = fila[3].strip()\n",
        "            local = fila[5].strip()\n",
        "            visita = fila[6].strip()\n",
        "        elif len(fila) >= 4:\n",
        "            score = fila[2].strip()\n",
        "            equipos = fila[3].strip()\n",
        "            partes = pat_equipo.split(equipos, maxsplit=1)\n",
        "            if len(partes) != 2:\n",
        "                continue\n",
        "            local, visita = partes[0].strip(), partes[1].strip()\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        g_local, g_visita = [int(x.strip()) for x in score.split(\"-\", 1)]\n",
        "\n",
        "        if g_local > g_visita:\n",
        "            p_local, p_visita = 3, 0\n",
        "        elif g_local < g_visita:\n",
        "            p_local, p_visita = 0, 3\n",
        "        else:\n",
        "            p_local, p_visita = 1, 1\n",
        "\n",
        "        print(f\"{local}\\t{p_local}\")\n",
        "        print(f\"{visita}\\t{p_visita}\")\n",
        "\n",
        "    except Exception:\n",
        "        # Ignoro líneas con formato inesperado para no tumbar el job.\n",
        "        continue\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jW1W0mlvkdcb",
        "outputId": "ec4f8660-545e-4907-dc22-c35de7ff06b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mapperPuntosEquipo.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Combiner**\n",
        "* Recibe par **<equipo, puntos>** ya ordenados por equipo, y acumula puntos por equipo.\n",
        "* Emite **<equipo, suma_parcial>** para optimizar lo que viaja al Reducer.\n"
      ],
      "metadata": {
        "id": "GqUo2sfnkSc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile combinerPuntosEquipo.py\n",
        "#!/usr/bin/python3\n",
        "import sys\n",
        "\n",
        "equipo_actual = None\n",
        "suma_actual = 0\n",
        "\n",
        "for linea in sys.stdin:\n",
        "    linea = linea.strip()\n",
        "    if not linea:\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        equipo, pts = linea.split(\"\\t\", 1)\n",
        "        pts = int(pts)\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "    if equipo_actual is None:\n",
        "        equipo_actual = equipo\n",
        "        suma_actual = pts\n",
        "        continue\n",
        "\n",
        "    if equipo == equipo_actual:\n",
        "        suma_actual += pts\n",
        "    else:\n",
        "        print(f\"{equipo_actual}\\t{suma_actual}\")\n",
        "        equipo_actual = equipo\n",
        "        suma_actual = pts\n",
        "\n",
        "if equipo_actual is not None:\n",
        "    print(f\"{equipo_actual}\\t{suma_actual}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJqOJkcXlpYn",
        "outputId": "6973503c-0102-4b88-8d7e-c76430766a27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing combinerPuntosEquipo.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Reducer**\n",
        "* Recibe **<equipo, suma_parcial>** y hace la suma final por equipo.\n",
        "* Emite el formato final solicitado: **equipo;puntos**.\n"
      ],
      "metadata": {
        "id": "J4vra2ojkU8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reducerPuntosEquipo.py\n",
        "#!/usr/bin/python3\n",
        "import sys\n",
        "\n",
        "equipo_actual = None\n",
        "suma_actual = 0\n",
        "\n",
        "for linea in sys.stdin:\n",
        "    linea = linea.strip()\n",
        "    if not linea:\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        equipo, pts = linea.split(\"\\t\", 1)\n",
        "        pts = int(pts)\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "    if equipo_actual is None:\n",
        "        equipo_actual = equipo\n",
        "        suma_actual = pts\n",
        "        continue\n",
        "\n",
        "    if equipo == equipo_actual:\n",
        "        suma_actual += pts\n",
        "    else:\n",
        "        print(f\"{equipo_actual};{suma_actual}\")\n",
        "        equipo_actual = equipo\n",
        "        suma_actual = pts\n",
        "\n",
        "if equipo_actual is not None:\n",
        "    print(f\"{equipo_actual};{suma_actual}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SP6r_cRzlTjJ",
        "outputId": "6f123fdb-4f7a-4f53-8f64-ea67d2f47fef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing reducerPuntosEquipo.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Permisos de ejecución\n",
        "Dar permisos al usuario de consola para ejecutar cada archivo."
      ],
      "metadata": {
        "id": "lHiEALnvm9nb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod u+x mapperPuntosEquipo.py\n",
        "!chmod u+x combinerPuntosEquipo.py\n",
        "!chmod u+x reducerPuntosEquipo.py"
      ],
      "metadata": {
        "id": "kWA735ucm8MO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Hadoop Streaming**\n",
        "Se fuerza 1 reducer para que la salida mantenga el orden global por clave (alfabético)."
      ],
      "metadata": {
        "id": "YnOCPwdjlUDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf salidaPuntosEquipo\n",
        "\n",
        "!hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.4.2.jar \\\n",
        "  -D mapreduce.job.reduces=1 \\\n",
        "  -file ./mapperPuntosEquipo.py -mapper ./mapperPuntosEquipo.py \\\n",
        "  -file ./combinerPuntosEquipo.py -combiner ./combinerPuntosEquipo.py \\\n",
        "  -file ./reducerPuntosEquipo.py -reducer ./reducerPuntosEquipo.py \\\n",
        "  -input resultados_futbol.csv -output ./salidaPuntosEquipo\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SahKjgdzoEeM",
        "outputId": "81da359c-fa68-4d57-9cdf-4c1c372c8c8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-11 15:16:35,946 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [./mapperPuntosEquipo.py, ./combinerPuntosEquipo.py, ./reducerPuntosEquipo.py] [] /tmp/streamjob746556288062757493.jar tmpDir=null\n",
            "2026-01-11 15:16:37,657 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2026-01-11 15:16:38,043 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2026-01-11 15:16:38,074 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2026-01-11 15:16:38,633 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1823042517_0001\n",
            "2026-01-11 15:16:38,635 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2026-01-11 15:16:39,315 INFO mapred.LocalDistributedCacheManager: Localized file:/content/mapperPuntosEquipo.py as file:/tmp/hadoop-root/mapred/local/job_local1823042517_0001_a8480874-1dea-4079-91c5-1c3f786406cf/mapperPuntosEquipo.py\n",
            "2026-01-11 15:16:39,380 INFO mapred.LocalDistributedCacheManager: Localized file:/content/combinerPuntosEquipo.py as file:/tmp/hadoop-root/mapred/local/job_local1823042517_0001_6fa414a2-2a1f-4fc5-86c2-e629b066104c/combinerPuntosEquipo.py\n",
            "2026-01-11 15:16:39,381 INFO mapred.LocalDistributedCacheManager: Localized file:/content/reducerPuntosEquipo.py as file:/tmp/hadoop-root/mapred/local/job_local1823042517_0001_d30e2811-b681-45df-8c5e-1270f7b63355/reducerPuntosEquipo.py\n",
            "2026-01-11 15:16:39,557 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2026-01-11 15:16:39,560 INFO mapreduce.Job: Running job: job_local1823042517_0001\n",
            "2026-01-11 15:16:39,560 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2026-01-11 15:16:39,566 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2026-01-11 15:16:39,582 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-01-11 15:16:39,582 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-01-11 15:16:39,645 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2026-01-11 15:16:39,648 INFO mapred.LocalJobRunner: Starting task: attempt_local1823042517_0001_m_000000_0\n",
            "2026-01-11 15:16:39,692 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-01-11 15:16:39,692 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-01-11 15:16:39,748 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2026-01-11 15:16:39,763 INFO mapred.MapTask: Processing split: file:/content/resultados_futbol.csv:0+569436\n",
            "2026-01-11 15:16:39,792 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2026-01-11 15:16:40,078 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2026-01-11 15:16:40,079 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2026-01-11 15:16:40,079 INFO mapred.MapTask: soft limit at 83886080\n",
            "2026-01-11 15:16:40,079 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2026-01-11 15:16:40,079 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2026-01-11 15:16:40,084 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2026-01-11 15:16:40,093 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./mapperPuntosEquipo.py]\n",
            "2026-01-11 15:16:40,099 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2026-01-11 15:16:40,100 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2026-01-11 15:16:40,101 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2026-01-11 15:16:40,101 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2026-01-11 15:16:40,102 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2026-01-11 15:16:40,102 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2026-01-11 15:16:40,104 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2026-01-11 15:16:40,105 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2026-01-11 15:16:40,105 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2026-01-11 15:16:40,106 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2026-01-11 15:16:40,107 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2026-01-11 15:16:40,111 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2026-01-11 15:16:40,156 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-01-11 15:16:40,156 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-01-11 15:16:40,160 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-01-11 15:16:40,180 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-01-11 15:16:40,236 INFO streaming.PipeMapRed: Records R/W=2064/1\n",
            "2026-01-11 15:16:40,387 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2026-01-11 15:16:40,388 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2026-01-11 15:16:40,393 INFO mapred.LocalJobRunner: \n",
            "2026-01-11 15:16:40,393 INFO mapred.MapTask: Starting flush of map output\n",
            "2026-01-11 15:16:40,393 INFO mapred.MapTask: Spilling map output\n",
            "2026-01-11 15:16:40,393 INFO mapred.MapTask: bufstart = 0; bufend = 213370; bufvoid = 104857600\n",
            "2026-01-11 15:16:40,393 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26141440(104565760); length = 72957/6553600\n",
            "2026-01-11 15:16:40,471 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./combinerPuntosEquipo.py]\n",
            "2026-01-11 15:16:40,478 INFO Configuration.deprecation: mapred.skip.map.auto.incr.proc.count is deprecated. Instead, use mapreduce.map.skip.proc-count.auto-incr\n",
            "2026-01-11 15:16:40,496 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-01-11 15:16:40,497 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-01-11 15:16:40,499 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-01-11 15:16:40,524 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-01-11 15:16:40,558 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-01-11 15:16:40,578 INFO mapreduce.Job: Job job_local1823042517_0001 running in uber mode : false\n",
            "2026-01-11 15:16:40,582 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2026-01-11 15:16:40,599 INFO streaming.PipeMapRed: Records R/W=18240/1\n",
            "2026-01-11 15:16:40,608 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2026-01-11 15:16:40,611 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2026-01-11 15:16:40,611 INFO mapred.MapTask: Finished spill 0\n",
            "2026-01-11 15:16:40,629 INFO mapred.Task: Task:attempt_local1823042517_0001_m_000000_0 is done. And is in the process of committing\n",
            "2026-01-11 15:16:40,634 INFO mapred.LocalJobRunner: Records R/W=18240/1\n",
            "2026-01-11 15:16:40,634 INFO mapred.Task: Task 'attempt_local1823042517_0001_m_000000_0' done.\n",
            "2026-01-11 15:16:40,646 INFO mapred.Task: Final Counters for attempt_local1823042517_0001_m_000000_0: Counters: 18\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=574033\n",
            "\t\tFILE: Number of bytes written=727102\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=9121\n",
            "\t\tMap output records=18240\n",
            "\t\tMap output bytes=213370\n",
            "\t\tMap output materialized bytes=709\n",
            "\t\tInput split bytes=87\n",
            "\t\tCombine input records=18240\n",
            "\t\tCombine output records=44\n",
            "\t\tSpilled Records=44\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=22\n",
            "\t\tTotal committed heap usage (bytes)=310378496\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=569436\n",
            "2026-01-11 15:16:40,646 INFO mapred.LocalJobRunner: Finishing task: attempt_local1823042517_0001_m_000000_0\n",
            "2026-01-11 15:16:40,646 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2026-01-11 15:16:40,651 INFO mapred.LocalJobRunner: Starting task: attempt_local1823042517_0001_r_000000_0\n",
            "2026-01-11 15:16:40,651 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2026-01-11 15:16:40,660 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-01-11 15:16:40,660 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-01-11 15:16:40,661 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2026-01-11 15:16:40,664 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@743438df\n",
            "2026-01-11 15:16:40,666 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2026-01-11 15:16:40,689 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2381106432, maxSingleShuffleLimit=595276608, mergeThreshold=1571530368, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2026-01-11 15:16:40,692 INFO reduce.EventFetcher: attempt_local1823042517_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2026-01-11 15:16:40,744 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1823042517_0001_m_000000_0 decomp: 705 len: 709 to MEMORY\n",
            "2026-01-11 15:16:40,750 INFO reduce.InMemoryMapOutput: Read 705 bytes from map-output for attempt_local1823042517_0001_m_000000_0\n",
            "2026-01-11 15:16:40,752 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 705, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->705\n",
            "2026-01-11 15:16:40,759 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2026-01-11 15:16:40,760 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2026-01-11 15:16:40,761 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2026-01-11 15:16:40,770 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2026-01-11 15:16:40,770 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 695 bytes\n",
            "2026-01-11 15:16:40,772 INFO reduce.MergeManagerImpl: Merged 1 segments, 705 bytes to disk to satisfy reduce memory limit\n",
            "2026-01-11 15:16:40,773 INFO reduce.MergeManagerImpl: Merging 1 files, 709 bytes from disk\n",
            "2026-01-11 15:16:40,774 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2026-01-11 15:16:40,774 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2026-01-11 15:16:40,775 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 695 bytes\n",
            "2026-01-11 15:16:40,776 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2026-01-11 15:16:40,783 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./reducerPuntosEquipo.py]\n",
            "2026-01-11 15:16:40,787 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2026-01-11 15:16:40,790 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2026-01-11 15:16:40,815 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-01-11 15:16:40,815 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-01-11 15:16:40,842 INFO streaming.PipeMapRed: Records R/W=44/1\n",
            "2026-01-11 15:16:40,850 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2026-01-11 15:16:40,850 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2026-01-11 15:16:40,852 INFO mapred.Task: Task:attempt_local1823042517_0001_r_000000_0 is done. And is in the process of committing\n",
            "2026-01-11 15:16:40,853 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2026-01-11 15:16:40,854 INFO mapred.Task: Task attempt_local1823042517_0001_r_000000_0 is allowed to commit now\n",
            "2026-01-11 15:16:40,856 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1823042517_0001_r_000000_0' to file:/content/salidaPuntosEquipo\n",
            "2026-01-11 15:16:40,858 INFO mapred.LocalJobRunner: Records R/W=44/1 > reduce\n",
            "2026-01-11 15:16:40,858 INFO mapred.Task: Task 'attempt_local1823042517_0001_r_000000_0' done.\n",
            "2026-01-11 15:16:40,860 INFO mapred.Task: Final Counters for attempt_local1823042517_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=575483\n",
            "\t\tFILE: Number of bytes written=728486\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=44\n",
            "\t\tReduce shuffle bytes=709\n",
            "\t\tReduce input records=44\n",
            "\t\tReduce output records=44\n",
            "\t\tSpilled Records=44\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=310378496\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=675\n",
            "2026-01-11 15:16:40,861 INFO mapred.LocalJobRunner: Finishing task: attempt_local1823042517_0001_r_000000_0\n",
            "2026-01-11 15:16:40,861 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2026-01-11 15:16:41,589 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2026-01-11 15:16:41,590 INFO mapreduce.Job: Job job_local1823042517_0001 completed successfully\n",
            "2026-01-11 15:16:41,600 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1149516\n",
            "\t\tFILE: Number of bytes written=1455588\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=9121\n",
            "\t\tMap output records=18240\n",
            "\t\tMap output bytes=213370\n",
            "\t\tMap output materialized bytes=709\n",
            "\t\tInput split bytes=87\n",
            "\t\tCombine input records=18240\n",
            "\t\tCombine output records=44\n",
            "\t\tReduce input groups=44\n",
            "\t\tReduce shuffle bytes=709\n",
            "\t\tReduce input records=44\n",
            "\t\tReduce output records=44\n",
            "\t\tSpilled Records=88\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=22\n",
            "\t\tTotal committed heap usage (bytes)=620756992\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=569436\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=675\n",
            "2026-01-11 15:16:41,600 INFO streaming.StreamJob: Output directory: ./salidaPuntosEquipo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat ./salidaPuntosEquipo/part-*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1Na4do-oGhZ",
        "outputId": "fe86d699-37d5-41f4-cc5c-759948fc550f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alavés;467\t\n",
            "Albacete;75\t\n",
            "Almería;283\t\n",
            "Athletic;1242\t\n",
            "Atlético;1463\t\n",
            "Barcelona;1965\t\n",
            "Celta;886\t\n",
            "Cádiz;161\t\n",
            "Córdoba;20\t\n",
            "Deportivo;883\t\n",
            "Deportivo Alavés;31\t\n",
            "Eibar;302\t\n",
            "Elche;184\t\n",
            "Espanyol;1055\t\n",
            "Getafe;840\t\n",
            "Gimnàstic Tarragona;28\t\n",
            "Girona;137\t\n",
            "Granada;359\t\n",
            "Huesca;67\t\n",
            "Hércules;35\t\n",
            "Las Palmas;191\t\n",
            "Leganés;159\t\n",
            "Levante;583\t\n",
            "Mallorca;825\t\n",
            "Málaga;791\t\n",
            "Numancia;148\t\n",
            "Osasuna;865\t\n",
            "R. Sociedad;973\t\n",
            "Racing;524\t\n",
            "Rayo Vallecano;525\t\n",
            "Real Betis;989\t\n",
            "Real Madrid;1959\t\n",
            "Real Murcia;56\t\n",
            "Real Oviedo;86\t\n",
            "Real Sociedad;133\t\n",
            "Real Sporting;237\t\n",
            "Real Valladolid;587\t\n",
            "Real Zaragoza;551\t\n",
            "Recreativo;167\t\n",
            "Sevilla;1386\t\n",
            "Tenerife;74\t\n",
            "Valencia;1449\t\n",
            "Villarreal;1280\t\n",
            "Xerez;34\t\n"
          ]
        }
      ]
    }
  ]
}