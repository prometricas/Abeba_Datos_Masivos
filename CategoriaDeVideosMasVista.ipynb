{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "yJHa4RzjhMa_",
        "15V6Nt-khqRw"
      ],
      "authorship_tag": "ABX9TyNENHYs+6Ltx7w22k/KO9oY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prometricas/Abeba_Datos_Masivos/blob/main/CategoriaDeVideosMasVista.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Instalación de Java y Spark**"
      ],
      "metadata": {
        "id": "yJHa4RzjhMa_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Instalar JDK de Java**"
      ],
      "metadata": {
        "id": "GQg7tQjVuobh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y openjdk-11-jdk-headless -qq > /dev/null"
      ],
      "metadata": {
        "id": "82PB4Zodu93s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Instalar Spark**"
      ],
      "metadata": {
        "id": "84JwNdLxwgQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargar Spark\n",
        "!wget https://dlcdn.apache.org/spark/spark-3.5.7/spark-3.5.7-bin-hadoop3.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoFWj1DUwuPz",
        "outputId": "1b86987f-faf0-453b-d68b-2826ff03328e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-01-13 13:36:30--  https://dlcdn.apache.org/spark/spark-3.5.7/spark-3.5.7-bin-hadoop3.tgz\n",
            "Resolving dlcdn.apache.org (dlcdn.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
            "Connecting to dlcdn.apache.org (dlcdn.apache.org)|151.101.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 400914067 (382M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.5.7-bin-hadoop3.tgz’\n",
            "\n",
            "spark-3.5.7-bin-had 100%[===================>] 382.34M   236MB/s    in 1.6s    \n",
            "\n",
            "2026-01-13 13:36:34 (236 MB/s) - ‘spark-3.5.7-bin-hadoop3.tgz’ saved [400914067/400914067]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Descomprimir\n",
        "!tar xf spark-3.5.7-bin-hadoop3.tgz\n",
        "\n",
        "# Mostrar folder descomprimido\n",
        "!ls /content"
      ],
      "metadata": {
        "id": "QxAAWIYeI6KP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92116cda-b32f-4d05-b655-d3e38b25831b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0302\t  sample_data\t\t   spark-3.5.7-bin-hadoop3.tgz\n",
            "0302.zip  spark-3.5.7-bin-hadoop3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Definir variables de entorno**\n"
      ],
      "metadata": {
        "id": "8tm2Ds-HJ6bF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.7-bin-hadoop3\""
      ],
      "metadata": {
        "id": "bl45zkpNKHI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Instalar e Iniciar findspark**\n",
        "Esto permite usar *PySpark* como una librería estándar en Python."
      ],
      "metadata": {
        "id": "mkLSx-oWC5gZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install findspark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k91s2zbhC97N",
        "outputId": "19ac5983-87dc-448f-8037-ce358077c903"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Importar carpeta con datos**\n",
        "Descarga carpeta, desde URL dada, y la descomprime."
      ],
      "metadata": {
        "id": "15V6Nt-khqRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "from tqdm import tqdm     # Librería para la barra de progreso\n",
        "\n",
        "# Limpio ejecuciones previas\n",
        "NOMBRE_ZIP = \"0302.zip\"\n",
        "DATASET = NOMBRE_ZIP.split(\".\")[0]\n",
        "! rm -rf {NOMBRE_ZIP}\n",
        "! rm -rf {DATASET}\n",
        "\n",
        "# Configuración\n",
        "URL_BASE = \"https://netsg.cs.sfu.ca/youtubedata/\"\n",
        "URL_COMPLETA = URL_BASE + NOMBRE_ZIP\n",
        "\n",
        "# Defino función de descarga\n",
        "def descargar_con_progreso(url, nombre_archivo):\n",
        "    # Iniciamos la conexión con stream=True para bajar por pedazos\n",
        "    response = requests.get(url, stream=True)\n",
        "    total_size_in_bytes = int(response.headers.get('content-length', 0))\n",
        "    block_size = 1024 # 1 Kibibyte\n",
        "\n",
        "    # Configuramos la barra de progreso\n",
        "    progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)\n",
        "\n",
        "    with open(nombre_archivo, 'wb') as file:\n",
        "        for data in response.iter_content(block_size):\n",
        "            progress_bar.update(len(data))\n",
        "            file.write(data)\n",
        "    progress_bar.close()\n",
        "\n",
        "    if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n",
        "        print(\"ERROR: Algo salió mal con la descarga.\")\n",
        "    else:\n",
        "        print(f\"\\n¡Descarga de {nombre_archivo} completada con éxito!\")\n",
        "\n",
        "\n",
        "# Ejecuto función para descargar carpeta comprimida\n",
        "print(f\"Iniciando descarga de {NOMBRE_ZIP}...\")\n",
        "descargar_con_progreso(URL_COMPLETA, NOMBRE_ZIP)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uSQQLZH4h7mM",
        "outputId": "463ca144-7617-4c07-b0f4-b676f20b7a3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando descarga de 0302.zip...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Descomprimir (Usamos -q para 'quiet' y evitar imprimir miles de líneas)\n",
        "print(\"Descomprimiendo archivos (esto puede tardar unos segundos)...\")\n",
        "!unzip -q -o {NOMBRE_ZIP}\n",
        "print(\"¡Descompresión finalizada!\")\n",
        "\n",
        "# Verificar\n",
        "print(\"\\nContenido de la carpeta:\")\n",
        "!ls {CARPETA_DESTINO} | head -5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iq0XJhPhHqm",
        "outputId": "d45b397f-4689-40d6-afcf-dcfea8cace76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descomprimiendo archivos (esto puede tardar unos segundos)...\n",
            "¡Descompresión finalizada!\n",
            "\n",
            "Contenido de la carpeta:\n",
            "0.txt\n",
            "1.txt\n",
            "2.txt\n",
            "log.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Crear script**\n",
        "* Parsea cada línea del dataset de Youtube.\n",
        "* Formato esperado (tabulado): *videoID, uploader, age, category, length, views, ...*\n",
        "* Indices: 0, 1, 2, 3, 4, 5\n",
        "\n",
        "Además, en lugar de leer todo asterisco, usamos \"[0-9]*.txt\". Esto le dice a Spark: \"Lee cualquier .txt que empiece con un número (0-9)\". Con esto filtramos automáticamente el 'log.txt' y leemos solo 0.txt, 1.txt, etc."
      ],
      "metadata": {
        "id": "fF33vO1kjjDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile CategoriaDeVideosMasVista.py\n",
        "import sys\n",
        "from pyspark import SparkContext\n",
        "\n",
        "# Inicializamos SparkContext\n",
        "sc = SparkContext(\"local\", \"YoutubeCategoryAnalysis\")\n",
        "\n",
        "# Argumentos: carpeta de entrada y carpeta de salida\n",
        "ruta_entrada = sys.argv[1]\n",
        "ruta_salida = sys.argv[2]\n",
        "\n",
        "def procesar_video(linea):\n",
        "    try:\n",
        "        partes = linea.split('\\t')\n",
        "\n",
        "        # Verificamos que la línea tenga suficientes columnas\n",
        "        if len(partes) > 5:\n",
        "            categoria = partes[3].strip()\n",
        "            visitas = int(partes[5].strip())\n",
        "            return (categoria, visitas)\n",
        "        else:\n",
        "            return None\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "try:\n",
        "    patron_archivos = ruta_entrada + \"/[0-9]*.txt\"\n",
        "    datos_rdd = sc.textFile(patron_archivos)\n",
        "\n",
        "    # Transformaciones:\n",
        "    # 1. Map: Extraer (Categoria, Visitas)\n",
        "    # 2. Filter: Quitar líneas corruptas (None)\n",
        "    # 3. ReduceByKey: Sumar visitas por categoría\n",
        "    categorias_vistas = datos_rdd.map(procesar_video) \\\n",
        "                                 .filter(lambda x: x is not None) \\\n",
        "                                 .reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "    categorias_vistas.cache()\n",
        "\n",
        "    # Encontrar la categoría ganadora para mostrar en consola\n",
        "    if not categorias_vistas.isEmpty():\n",
        "        categoria_top = categorias_vistas.max(key=lambda x: x[1])\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(f\"RESULTADO FINAL: La categoría más vista es '{categoria_top[0]}'\")\n",
        "        print(f\"Total de visualizaciones: {categoria_top[1]}\")\n",
        "        print(\"=\"*50 + \"\\n\")\n",
        "    else:\n",
        "        print(\"No se encontraron datos válidos.\")\n",
        "\n",
        "    # Guardamos los resultados\n",
        "    salida_formateada = categorias_vistas.map(lambda x: \"{}; {}\".format(x[0], x[1]))\n",
        "    salida_formateada.saveAsTextFile(ruta_salida)\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error durante la ejecución:\", e)\n",
        "\n",
        "finally:\n",
        "    sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nr3e62qwjjnv",
        "outputId": "c4bdeeea-6fc6-441d-af7c-59dc185f3e5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing CategoriaDeVideosMasVista.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Ejecutar**"
      ],
      "metadata": {
        "id": "jdBFQoifkUyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Borrar la carpeta de salida de Spark si existe, no la de entrada de datos\n",
        "!rm -rf resultado_youtube\n",
        "\n",
        "# Construir el comando spark-submit asegurando que la variable DATASET se interpola correctamente.\n",
        "# Usamos un f-string para asegurarnos de que {DATASET} se reemplace con su valor ('0302').\n",
        "spark_submit_command = f\"{os.environ['SPARK_HOME']}/bin/spark-submit CategoriaDeVideosMasVista.py {DATASET} resultado_youtube\"\n",
        "\n",
        "# Ejecutar el comando\n",
        "print(f\"Ejecutando: {spark_submit_command}\")\n",
        "!{spark_submit_command}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_BP6hr_nNe6",
        "outputId": "88110db4-7422-46bf-b342-2c77c626173e",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ejecutando: /content/spark-3.5.7-bin-hadoop3/bin/spark-submit CategoriaDeVideosMasVista.py 0302 resultado_youtube\n",
            "26/01/13 14:08:53 INFO SparkContext: Running Spark version 3.5.7\n",
            "26/01/13 14:08:53 INFO SparkContext: OS info Linux, 6.6.105+, amd64\n",
            "26/01/13 14:08:53 INFO SparkContext: Java version 17.0.17\n",
            "26/01/13 14:08:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "26/01/13 14:08:53 INFO ResourceUtils: ==============================================================\n",
            "26/01/13 14:08:53 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "26/01/13 14:08:53 INFO ResourceUtils: ==============================================================\n",
            "26/01/13 14:08:53 INFO SparkContext: Submitted application: YoutubeCategoryAnalysis\n",
            "26/01/13 14:08:53 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "26/01/13 14:08:53 INFO ResourceProfile: Limiting resource is cpu\n",
            "26/01/13 14:08:53 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "26/01/13 14:08:54 INFO SecurityManager: Changing view acls to: root\n",
            "26/01/13 14:08:54 INFO SecurityManager: Changing modify acls to: root\n",
            "26/01/13 14:08:54 INFO SecurityManager: Changing view acls groups to: \n",
            "26/01/13 14:08:54 INFO SecurityManager: Changing modify acls groups to: \n",
            "26/01/13 14:08:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "26/01/13 14:08:54 INFO Utils: Successfully started service 'sparkDriver' on port 42763.\n",
            "26/01/13 14:08:54 INFO SparkEnv: Registering MapOutputTracker\n",
            "26/01/13 14:08:54 INFO SparkEnv: Registering BlockManagerMaster\n",
            "26/01/13 14:08:54 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "26/01/13 14:08:54 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "26/01/13 14:08:54 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "26/01/13 14:08:54 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-51fcd220-69dc-4d4d-be86-dd5da300d976\n",
            "26/01/13 14:08:54 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
            "26/01/13 14:08:54 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "26/01/13 14:08:55 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "26/01/13 14:08:55 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "26/01/13 14:08:55 INFO Executor: Starting executor ID driver on host f447c10a57d9\n",
            "26/01/13 14:08:55 INFO Executor: OS info Linux, 6.6.105+, amd64\n",
            "26/01/13 14:08:55 INFO Executor: Java version 17.0.17\n",
            "26/01/13 14:08:55 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "26/01/13 14:08:55 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@4f82f90 for default.\n",
            "26/01/13 14:08:55 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46737.\n",
            "26/01/13 14:08:55 INFO NettyBlockTransferService: Server created on f447c10a57d9:46737\n",
            "26/01/13 14:08:55 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "26/01/13 14:08:55 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f447c10a57d9, 46737, None)\n",
            "26/01/13 14:08:55 INFO BlockManagerMasterEndpoint: Registering block manager f447c10a57d9:46737 with 434.4 MiB RAM, BlockManagerId(driver, f447c10a57d9, 46737, None)\n",
            "26/01/13 14:08:55 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f447c10a57d9, 46737, None)\n",
            "26/01/13 14:08:55 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f447c10a57d9, 46737, None)\n",
            "26/01/13 14:08:56 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 221.5 KiB, free 434.2 MiB)\n",
            "26/01/13 14:08:56 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 434.2 MiB)\n",
            "26/01/13 14:08:56 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f447c10a57d9:46737 (size: 32.6 KiB, free: 434.4 MiB)\n",
            "26/01/13 14:08:56 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
            "26/01/13 14:08:57 INFO FileInputFormat: Total input files to process : 3\n",
            "26/01/13 14:08:57 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
            "26/01/13 14:08:57 INFO DAGScheduler: Registering RDD 3 (reduceByKey at /content/CategoriaDeVideosMasVista.py:35) as input to shuffle 0\n",
            "26/01/13 14:08:57 INFO DAGScheduler: Got job 0 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
            "26/01/13 14:08:57 INFO DAGScheduler: Final stage: ResultStage 1 (runJob at PythonRDD.scala:181)\n",
            "26/01/13 14:08:57 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n",
            "26/01/13 14:08:57 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n",
            "26/01/13 14:08:57 INFO DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /content/CategoriaDeVideosMasVista.py:35), which has no missing parents\n",
            "26/01/13 14:08:57 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.2 KiB, free 434.1 MiB)\n",
            "26/01/13 14:08:57 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.6 KiB, free 434.1 MiB)\n",
            "26/01/13 14:08:57 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f447c10a57d9:46737 (size: 8.6 KiB, free: 434.4 MiB)\n",
            "26/01/13 14:08:57 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1611\n",
            "26/01/13 14:08:57 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /content/CategoriaDeVideosMasVista.py:35) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
            "26/01/13 14:08:57 INFO TaskSchedulerImpl: Adding task set 0.0 with 3 tasks resource profile 0\n",
            "26/01/13 14:08:57 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (f447c10a57d9, executor driver, partition 0, PROCESS_LOCAL, 9028 bytes) \n",
            "26/01/13 14:08:57 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "26/01/13 14:08:57 INFO HadoopRDD: Input split: file:/content/0302/0.txt:0+59091\n",
            "26/01/13 14:09:00 INFO PythonRunner: Times: total = 1564, boot = 1223, init = 339, finish = 2\n",
            "26/01/13 14:09:00 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1668 bytes result sent to driver\n",
            "26/01/13 14:09:00 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (f447c10a57d9, executor driver, partition 1, PROCESS_LOCAL, 9028 bytes) \n",
            "26/01/13 14:09:00 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
            "26/01/13 14:09:00 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2626 ms on f447c10a57d9 (executor driver) (1/3)\n",
            "26/01/13 14:09:00 INFO HadoopRDD: Input split: file:/content/0302/1.txt:0+456948\n",
            "26/01/13 14:09:00 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 53391\n",
            "26/01/13 14:09:00 INFO PythonRunner: Times: total = 294, boot = -781, init = 1065, finish = 10\n",
            "26/01/13 14:09:00 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1582 bytes result sent to driver\n",
            "26/01/13 14:09:00 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (f447c10a57d9, executor driver, partition 2, PROCESS_LOCAL, 9028 bytes) \n",
            "26/01/13 14:09:00 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)\n",
            "26/01/13 14:09:00 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 480 ms on f447c10a57d9 (executor driver) (2/3)\n",
            "26/01/13 14:09:00 INFO HadoopRDD: Input split: file:/content/0302/2.txt:0+2438282\n",
            "26/01/13 14:09:01 INFO PythonRunner: Times: total = 316, boot = -69, init = 320, finish = 65\n",
            "26/01/13 14:09:01 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 1625 bytes result sent to driver\n",
            "26/01/13 14:09:01 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 472 ms on f447c10a57d9 (executor driver) (3/3)\n",
            "26/01/13 14:09:01 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "26/01/13 14:09:01 INFO DAGScheduler: ShuffleMapStage 0 (reduceByKey at /content/CategoriaDeVideosMasVista.py:35) finished in 3.841 s\n",
            "26/01/13 14:09:01 INFO DAGScheduler: looking for newly runnable stages\n",
            "26/01/13 14:09:01 INFO DAGScheduler: running: Set()\n",
            "26/01/13 14:09:01 INFO DAGScheduler: waiting: Set(ResultStage 1)\n",
            "26/01/13 14:09:01 INFO DAGScheduler: failed: Set()\n",
            "26/01/13 14:09:01 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[7] at RDD at PythonRDD.scala:53), which has no missing parents\n",
            "26/01/13 14:09:01 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 11.5 KiB, free 434.1 MiB)\n",
            "26/01/13 14:09:01 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 434.1 MiB)\n",
            "26/01/13 14:09:01 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f447c10a57d9:46737 (size: 6.4 KiB, free: 434.4 MiB)\n",
            "26/01/13 14:09:01 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1611\n",
            "26/01/13 14:09:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (PythonRDD[7] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
            "26/01/13 14:09:01 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "26/01/13 14:09:01 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 3) (f447c10a57d9, executor driver, partition 0, NODE_LOCAL, 8817 bytes) \n",
            "26/01/13 14:09:01 INFO Executor: Running task 0.0 in stage 1.0 (TID 3)\n",
            "26/01/13 14:09:01 INFO ShuffleBlockFetcherIterator: Getting 3 (549.0 B) non-empty blocks including 3 (549.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "26/01/13 14:09:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 58 ms\n",
            "26/01/13 14:09:01 INFO PythonRunner: Times: total = 256, boot = -393, init = 648, finish = 1\n",
            "26/01/13 14:09:01 INFO MemoryStore: Block rdd_6_0 stored as values in memory (estimated size 196.0 B, free 434.1 MiB)\n",
            "26/01/13 14:09:01 INFO BlockManagerInfo: Added rdd_6_0 in memory on f447c10a57d9:46737 (size: 196.0 B, free: 434.4 MiB)\n",
            "26/01/13 14:09:01 INFO PythonRunner: Times: total = 85, boot = -2, init = 87, finish = 0\n",
            "26/01/13 14:09:01 INFO Executor: Finished task 0.0 in stage 1.0 (TID 3). 2118 bytes result sent to driver\n",
            "26/01/13 14:09:01 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 3) in 598 ms on f447c10a57d9 (executor driver) (1/1)\n",
            "26/01/13 14:09:01 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "26/01/13 14:09:01 INFO DAGScheduler: ResultStage 1 (runJob at PythonRDD.scala:181) finished in 0.674 s\n",
            "26/01/13 14:09:01 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "26/01/13 14:09:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "26/01/13 14:09:01 INFO DAGScheduler: Job 0 finished: runJob at PythonRDD.scala:181, took 4.700233 s\n",
            "26/01/13 14:09:02 INFO SparkContext: Starting job: max at /content/CategoriaDeVideosMasVista.py:41\n",
            "26/01/13 14:09:02 INFO DAGScheduler: Got job 1 (max at /content/CategoriaDeVideosMasVista.py:41) with 3 output partitions\n",
            "26/01/13 14:09:02 INFO DAGScheduler: Final stage: ResultStage 3 (max at /content/CategoriaDeVideosMasVista.py:41)\n",
            "26/01/13 14:09:02 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
            "26/01/13 14:09:02 INFO DAGScheduler: Missing parents: List()\n",
            "26/01/13 14:09:02 INFO DAGScheduler: Submitting ResultStage 3 (PythonRDD[8] at max at /content/CategoriaDeVideosMasVista.py:41), which has no missing parents\n",
            "26/01/13 14:09:02 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 12.6 KiB, free 434.1 MiB)\n",
            "26/01/13 14:09:02 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.6 KiB, free 434.1 MiB)\n",
            "26/01/13 14:09:02 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on f447c10a57d9:46737 (size: 6.6 KiB, free: 434.3 MiB)\n",
            "26/01/13 14:09:02 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1611\n",
            "26/01/13 14:09:02 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 3 (PythonRDD[8] at max at /content/CategoriaDeVideosMasVista.py:41) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
            "26/01/13 14:09:02 INFO TaskSchedulerImpl: Adding task set 3.0 with 3 tasks resource profile 0\n",
            "26/01/13 14:09:02 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 4) (f447c10a57d9, executor driver, partition 0, PROCESS_LOCAL, 8817 bytes) \n",
            "26/01/13 14:09:02 INFO Executor: Running task 0.0 in stage 3.0 (TID 4)\n",
            "26/01/13 14:09:02 INFO BlockManager: Found block rdd_6_0 locally\n",
            "26/01/13 14:09:02 INFO BlockManagerInfo: Removed broadcast_2_piece0 on f447c10a57d9:46737 in memory (size: 6.4 KiB, free: 434.4 MiB)\n",
            "26/01/13 14:09:02 INFO PythonRunner: Times: total = 182, boot = 6, init = 175, finish = 1\n",
            "26/01/13 14:09:02 INFO Executor: Finished task 0.0 in stage 3.0 (TID 4). 1422 bytes result sent to driver\n",
            "26/01/13 14:09:02 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 5) (f447c10a57d9, executor driver, partition 1, NODE_LOCAL, 8817 bytes) \n",
            "26/01/13 14:09:02 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 4) in 237 ms on f447c10a57d9 (executor driver) (1/3)\n",
            "26/01/13 14:09:02 INFO Executor: Running task 1.0 in stage 3.0 (TID 5)\n",
            "26/01/13 14:09:02 INFO ShuffleBlockFetcherIterator: Getting 3 (513.0 B) non-empty blocks including 3 (513.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "26/01/13 14:09:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
            "26/01/13 14:09:02 INFO PythonRunner: Times: total = 119, boot = 3, init = 116, finish = 0\n",
            "26/01/13 14:09:02 INFO MemoryStore: Block rdd_6_1 stored as values in memory (estimated size 189.0 B, free 434.1 MiB)\n",
            "26/01/13 14:09:02 INFO BlockManagerInfo: Added rdd_6_1 in memory on f447c10a57d9:46737 (size: 189.0 B, free: 434.4 MiB)\n",
            "26/01/13 14:09:02 INFO PythonRunner: Times: total = 129, boot = 15, init = 114, finish = 0\n",
            "26/01/13 14:09:02 INFO Executor: Finished task 1.0 in stage 3.0 (TID 5). 2111 bytes result sent to driver\n",
            "26/01/13 14:09:02 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 6) (f447c10a57d9, executor driver, partition 2, NODE_LOCAL, 8817 bytes) \n",
            "26/01/13 14:09:02 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 5) in 377 ms on f447c10a57d9 (executor driver) (2/3)\n",
            "26/01/13 14:09:02 INFO Executor: Running task 2.0 in stage 3.0 (TID 6)\n",
            "26/01/13 14:09:02 INFO ShuffleBlockFetcherIterator: Getting 3 (375.0 B) non-empty blocks including 3 (375.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "26/01/13 14:09:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "26/01/13 14:09:02 INFO PythonRunner: Times: total = 119, boot = -8, init = 127, finish = 0\n",
            "26/01/13 14:09:02 INFO MemoryStore: Block rdd_6_2 stored as values in memory (estimated size 139.0 B, free 434.1 MiB)\n",
            "26/01/13 14:09:02 INFO BlockManagerInfo: Added rdd_6_2 in memory on f447c10a57d9:46737 (size: 139.0 B, free: 434.4 MiB)\n",
            "26/01/13 14:09:02 INFO PythonRunner: Times: total = 98, boot = 16, init = 82, finish = 0\n",
            "26/01/13 14:09:02 INFO Executor: Finished task 2.0 in stage 3.0 (TID 6). 2108 bytes result sent to driver\n",
            "26/01/13 14:09:02 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 6) in 281 ms on f447c10a57d9 (executor driver) (3/3)\n",
            "26/01/13 14:09:02 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "26/01/13 14:09:02 INFO DAGScheduler: ResultStage 3 (max at /content/CategoriaDeVideosMasVista.py:41) finished in 0.949 s\n",
            "26/01/13 14:09:02 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "26/01/13 14:09:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "26/01/13 14:09:02 INFO DAGScheduler: Job 1 finished: max at /content/CategoriaDeVideosMasVista.py:41, took 0.973948 s\n",
            "\n",
            "==================================================\n",
            "RESULTADO FINAL: La categoría más vista es 'Music'\n",
            "Total de visualizaciones: 26922840\n",
            "==================================================\n",
            "\n",
            "26/01/13 14:09:03 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n",
            "26/01/13 14:09:03 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "26/01/13 14:09:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "26/01/13 14:09:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "26/01/13 14:09:03 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83\n",
            "26/01/13 14:09:03 INFO DAGScheduler: Got job 2 (runJob at SparkHadoopWriter.scala:83) with 3 output partitions\n",
            "26/01/13 14:09:03 INFO DAGScheduler: Final stage: ResultStage 5 (runJob at SparkHadoopWriter.scala:83)\n",
            "26/01/13 14:09:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\n",
            "26/01/13 14:09:03 INFO DAGScheduler: Missing parents: List()\n",
            "26/01/13 14:09:03 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[11] at saveAsTextFile at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "26/01/13 14:09:03 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 110.6 KiB, free 434.0 MiB)\n",
            "26/01/13 14:09:03 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 42.0 KiB, free 434.0 MiB)\n",
            "26/01/13 14:09:03 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on f447c10a57d9:46737 (size: 42.0 KiB, free: 434.3 MiB)\n",
            "26/01/13 14:09:03 INFO BlockManagerInfo: Removed broadcast_3_piece0 on f447c10a57d9:46737 in memory (size: 6.6 KiB, free: 434.3 MiB)\n",
            "26/01/13 14:09:03 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1611\n",
            "26/01/13 14:09:03 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at saveAsTextFile at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
            "26/01/13 14:09:03 INFO TaskSchedulerImpl: Adding task set 5.0 with 3 tasks resource profile 0\n",
            "26/01/13 14:09:03 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 7) (f447c10a57d9, executor driver, partition 0, PROCESS_LOCAL, 8817 bytes) \n",
            "26/01/13 14:09:03 INFO Executor: Running task 0.0 in stage 5.0 (TID 7)\n",
            "26/01/13 14:09:03 INFO BlockManager: Found block rdd_6_0 locally\n",
            "26/01/13 14:09:03 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "26/01/13 14:09:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "26/01/13 14:09:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "26/01/13 14:09:03 INFO PythonRunner: Times: total = 122, boot = -261, init = 383, finish = 0\n",
            "26/01/13 14:09:03 INFO FileOutputCommitter: Saved output of task 'attempt_202601131409033032662810866009092_0011_m_000000_0' to file:/content/resultado_youtube/_temporary/0/task_202601131409033032662810866009092_0011_m_000000\n",
            "26/01/13 14:09:03 INFO SparkHadoopMapRedUtil: attempt_202601131409033032662810866009092_0011_m_000000_0: Committed. Elapsed time: 9 ms.\n",
            "26/01/13 14:09:03 INFO Executor: Finished task 0.0 in stage 5.0 (TID 7). 1749 bytes result sent to driver\n",
            "26/01/13 14:09:03 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 8) (f447c10a57d9, executor driver, partition 1, PROCESS_LOCAL, 8817 bytes) \n",
            "26/01/13 14:09:03 INFO Executor: Running task 1.0 in stage 5.0 (TID 8)\n",
            "26/01/13 14:09:03 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 7) in 262 ms on f447c10a57d9 (executor driver) (1/3)\n",
            "26/01/13 14:09:03 INFO BlockManager: Found block rdd_6_1 locally\n",
            "26/01/13 14:09:03 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "26/01/13 14:09:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "26/01/13 14:09:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "26/01/13 14:09:03 INFO PythonRunner: Times: total = 115, boot = -24, init = 139, finish = 0\n",
            "26/01/13 14:09:03 INFO FileOutputCommitter: Saved output of task 'attempt_202601131409033032662810866009092_0011_m_000001_0' to file:/content/resultado_youtube/_temporary/0/task_202601131409033032662810866009092_0011_m_000001\n",
            "26/01/13 14:09:03 INFO SparkHadoopMapRedUtil: attempt_202601131409033032662810866009092_0011_m_000001_0: Committed. Elapsed time: 3 ms.\n",
            "26/01/13 14:09:03 INFO Executor: Finished task 1.0 in stage 5.0 (TID 8). 1792 bytes result sent to driver\n",
            "26/01/13 14:09:03 INFO TaskSetManager: Starting task 2.0 in stage 5.0 (TID 9) (f447c10a57d9, executor driver, partition 2, PROCESS_LOCAL, 8817 bytes) \n",
            "26/01/13 14:09:03 INFO Executor: Running task 2.0 in stage 5.0 (TID 9)\n",
            "26/01/13 14:09:03 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 8) in 164 ms on f447c10a57d9 (executor driver) (2/3)\n",
            "26/01/13 14:09:03 INFO BlockManager: Found block rdd_6_2 locally\n",
            "26/01/13 14:09:03 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "26/01/13 14:09:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "26/01/13 14:09:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "26/01/13 14:09:03 INFO PythonRunner: Times: total = 137, boot = -22, init = 159, finish = 0\n",
            "26/01/13 14:09:03 INFO FileOutputCommitter: Saved output of task 'attempt_202601131409033032662810866009092_0011_m_000002_0' to file:/content/resultado_youtube/_temporary/0/task_202601131409033032662810866009092_0011_m_000002\n",
            "26/01/13 14:09:03 INFO SparkHadoopMapRedUtil: attempt_202601131409033032662810866009092_0011_m_000002_0: Committed. Elapsed time: 4 ms.\n",
            "26/01/13 14:09:03 INFO Executor: Finished task 2.0 in stage 5.0 (TID 9). 1706 bytes result sent to driver\n",
            "26/01/13 14:09:03 INFO TaskSetManager: Finished task 2.0 in stage 5.0 (TID 9) in 178 ms on f447c10a57d9 (executor driver) (3/3)\n",
            "26/01/13 14:09:03 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
            "26/01/13 14:09:03 INFO DAGScheduler: ResultStage 5 (runJob at SparkHadoopWriter.scala:83) finished in 0.649 s\n",
            "26/01/13 14:09:03 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "26/01/13 14:09:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
            "26/01/13 14:09:03 INFO DAGScheduler: Job 2 finished: runJob at SparkHadoopWriter.scala:83, took 0.662296 s\n",
            "26/01/13 14:09:03 INFO SparkHadoopWriter: Start to commit write Job job_202601131409033032662810866009092_0011.\n",
            "26/01/13 14:09:03 INFO SparkHadoopWriter: Write Job job_202601131409033032662810866009092_0011 committed. Elapsed time: 19 ms.\n",
            "26/01/13 14:09:03 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "26/01/13 14:09:03 INFO SparkUI: Stopped Spark web UI at http://f447c10a57d9:4040\n",
            "26/01/13 14:09:03 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "26/01/13 14:09:03 INFO MemoryStore: MemoryStore cleared\n",
            "26/01/13 14:09:03 INFO BlockManager: BlockManager stopped\n",
            "26/01/13 14:09:03 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "26/01/13 14:09:03 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "26/01/13 14:09:03 INFO SparkContext: Successfully stopped SparkContext\n",
            "26/01/13 14:09:04 INFO ShutdownHookManager: Shutdown hook called\n",
            "26/01/13 14:09:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-b4d13d7e-f903-4bb4-8976-2282aea3aa90/pyspark-52ad0952-24ed-458e-b8a7-3182e3214178\n",
            "26/01/13 14:09:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-b4d13d7e-f903-4bb4-8976-2282aea3aa90\n",
            "26/01/13 14:09:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-828dbbfd-ba99-40c3-b962-bf1e8ed4bc62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head resultado_youtube/*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jj5s1mBKkjrj",
        "outputId": "97055ad9-f599-41b8-c229-18f3ca4ab0f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> resultado_youtube/part-00000 <==\n",
            "Entertainment; 19646440\n",
            "Music; 26922840\n",
            "Howto & DIY; 932642\n",
            "People & Blogs; 3648430\n",
            "Gadgets & Games; 2070193\n",
            "\n",
            "==> resultado_youtube/part-00001 <==\n",
            "Sports; 5370975\n",
            "Film & Animation; 10823098\n",
            "Comedy; 16626515\n",
            "Pets & Animals; 2212879\n",
            "Travel & Places; 305409\n",
            "\n",
            "==> resultado_youtube/part-00002 <==\n",
            "News & Politics; 3091377\n",
            "Autos & Vehicles; 1411860\n",
            "UNA; 4591983\n",
            "\n",
            "==> resultado_youtube/_SUCCESS <==\n"
          ]
        }
      ]
    }
  ]
}